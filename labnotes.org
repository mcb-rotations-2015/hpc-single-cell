* <2015-09-21 Mon> Checking job progress
- Background data 3-day run completed, with a couple that took 1+ days
  and were killed due to timeout.
- Need a way to easily record progress of how many jobs are run, and
  how many are remaining.
  - There are ~--parallel~ and ~--eta~ options for checking progress,
    although they would require assigning hostnames for ssh.  This
    would mean not using ~sbatch~, but ~salloc~ or something
    else which requires an explicit ~exit~ shell command.
  - Thus need to confirm a few things:
    - ~salloc~ sets ~SLURM_JOB_NODELIST~ in expanded rather than
      compact format (at least ~echo~ the variable).
    - ~--eta~ outputs progress to stdout recorded by SLURM (the
      ~slurm-JOBID.out~ file).
    - Confirm there is information about how many jobs are done vs
      complete.
- There is a bug in GNU Parallel 20150810 when using both ~--joblog~
  and ~--dryrun~ together.  It records the dryrun as successful in the
  joblog.  Therefore when removing dryrun, the jobs do not actually
  run since the dryrun simulations are listed in the log as having
  been successfully ran!  One has to remove the appropriate lines from
  joblog.
  - Reported issue on mailing list..
  - Cloned the Savannah git repository and will look into maybe
    patching this.

* <2015-09-17 Thu> Background MACSE and PAML
- Unpacking raw background data exceeds 2G limit of home directory.
  - Copied tarball of processed data ~results-macse-dn-sample.tar.xz~
    to lab server home directory (6.4 MB).
  - Moved data folder to ~/scratch/scratch0/$USER/~
  - Unpacked tarball of new background data (unpacking seems slower on
    ~/scratch~).
- There is a lot of common code between macse and paml execution.  A
  refactor is needed to:
  - Share the ~sbatch-*~ script.
  - Share the parallel section of ~parallel-*~ script.
  - Pass all sbatch parameters onto the macse/paml.
  - Reduce the number of folders, and moving scripts to the root
    directory
    - Move the dependencies to a root ~3rdparty~ directory.
    - Create a log with sub-directories for slurm, parallel, macse and
      paml.
  - Add Makefile rules for packing and unpacking data.
- Ran MACSE on background data.

* <2015-09-16 Wed> Makefiles and parallelize PAML
- Added Makefiles to install MACSE and PAML.
- Ajay confirmed PAML is also "embarrasingly parallel problem" (single
  cpu, single thread).  Read PAML documentation to understand purpose
  of ~.ctl~ files.
- MACSE fixes:
  - Fixed joblog naming.
  - Added loop for fast, low memory run, then high memory for failed,
    memory-intensive jobs.
- Submitted background MACSE files onto 2 nodes using 2 submissions.
  - Jobs failed since the background sequence folder was missing.

* <2015-09-15 Tue> Remaining dn_sample jobs
- 3 jobs ran out of memory even with ~java -Xmx4096~.
- Found bugs in my bash scripts.  Jobs were failing to rerun due to 2
  reasons:
  - Need to use ~--resume-failed~ instead of ~--resume~ parameter.
  - ~module~ program not being loaded.  Needed to use the hacky
    ~source /etc/profile.d/modules~ line to access the module program.
    - Why did it work on the previous run?  Probably the shell
      environment was being read by sbatch, somehow.
- Found that SLURM on the cluster ignores punishes any memory request:
  using ~--mem-per-cpu~ parameter for ~sbatch~ and ~srun~ causes jobs
  to immediately fail with some unable to allocate resource error.
  Tingyang Xu confirmed that this is because memory control has not
  been implemented for our system.  Therefore removed that requests
  parameter from the scheduler options.
- Learned new behavior of ~sbatch~: One must specify number of number
  of cpus per node, otherwise it only assigns one CPU in a node.  This
  should be documented on the BECAT user wiki.
- Removed ~srun~ from ~parallel-macse.sh~ because using ~srun~ is too
  limited: it does not allow control of memory or a dynamic view of
  available cpus.  ~srun~ also limits jobs to 11 GB, whereas more
  memory is available in the nodes.  The better approach is to
  architect the script to run on a single node, and then use
  parallel's intelligence to launch tasks depending on available
  memory and cpus.
  - Removing ~srun~ means that One loses the granularity of seeing the
    jobs in the ~sacct~.  One can instead see progress in ~top~ by
    finding out the execution node from ~sacct~ and then ssh-ing into
    the node and running ~top~:
    : top -M -u $(whoami)
  - Looking at top was very useful; the memory ~RES~ or resident size
    for most of the processes stay at about 1G, but one of the
    processes was at 17G and then grew to 20G (fasta file name
    ENSFM00250000001272).
  - Installed htop in my home directory to see the fasta filename,
    etc.
  - It probably makes sense to first sort the tsv file by the fasta
    input file size, to run fast, low-memory jobs first.  Would it be
    better not to destructively change the tsv file, but operate off
    of another tsv-sorted file?  Checked out the ENSFM00250000001272
    pa.fasta file size; it was only 20k compared to 315k of the
    largest file.  The pg.fasta file is 64k compared to the largest
    202k
    : ls -lhSr ~/test*/DN/*/*_pg.fasta
  - Ideally the java process should know exactly how much memory it
    needs.  Alternatively, there needs to be an OutOfMemory handler
    that will re-run a failed task, incrementally increasing the
    memory as needed.

* <2015-09-14 Mon> Remaining dn_sample jobs
- Applied Ajay suggestion of -Xmx2048
- Only 1 job left in the queue; slowness may be from swapping?
  Reduced requested tasks and reran to complete remaining jobs.

* <2015-09-13 Sun> Running dn_sample.tsv
- Completed scripts ~parallel-macse.sh~ and ~sbatch-macse.sh~.
  - Added support for bash parameters.
  - Added usage messages.
  - Issues in ~parallel-macse.sh~:
    - Each task output should redirect into separate files in a
      directory named after the SLURM job ID, since in a single file
      the output gets all jumbled up.
      - Need some way of mapping the SLURM job.batch ID to the
	individual log name.  This is probably the same as ~{#}~ of
	GNU Parallel.
      - Create new ~--output-dir~ option for ~parallel-macse.sh~.
    - joblog is named by the file suffix instead of its prefix.  Need
      to replace ~##~ with ~%%~ and transpose ~*.~ to ~.*~
      - Don't fix this bug until all jobs complete successfully.
- Encountered at least 4 Java OutOfMemory errors.  Added ~java
  -verbose:gc~ to see last garbage collection attempt before it runs
  out of memory to get an idea of much memory is available.  But,
  should first fix the issue of separate log files for this
  enhancement to be useful.
#+BEGIN_SRC sh
# Failed jobs
$ sacct -s R --brief | grep -v 0:0
       JobID      State ExitCode
------------ ---------- --------
24598.5          FAILED      1:0
24598.211        FAILED      1:0
24598.303        FAILED      1:0
24598.317        FAILED      1:0

# Can check on running jobs with:
$ sacct -j 24598 --format=jobid,state,elapsed,nodelist | grep -v COMPLETED
#+END_SRC

* <2015-09-12 Sat> GNU Parallel experimentation
- Read GNU Parallel man page.
- Ran GNU Parallel tutorial.
- Created [[https://www.becat.uconn.edu/wiki/index.php/Parallel_Guide][BECAT wiki guide]] for GNU Parallel.
- Refactored batch script to use builtin features of GNU Parallel.
  - Added ~--arg-file~ instead of mapping integers to the tsv rows.
  - Added ~--header~~ for named column selection.
  - Got rid of bash functions which were doing these things.
